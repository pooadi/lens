{"cells":[{"cell_type":"markdown","metadata":{"id":"2pXWrnPjRfZ-"},"source":["---\n","\n","\n","###***Initialisation cells (Run these cells to download the necessary libraries and mount the drive)***\n","\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yIVuBc8FRTM6"},"outputs":[],"source":["# code to mount my drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V2DSZtCpRnMV"},"outputs":[],"source":["%cd /content/drive/MyDrive/My_Software_Projects/lens\n","!pip install -Uqq ipdb\n","import ipdb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2UMNth0RrKF"},"outputs":[],"source":["%pdb on"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TIJUR1TJRudm"},"outputs":[],"source":["# install the necessary requirements\n","\n","!pip install Pillow datasets transformers llm-lens torch\n","!pip install SpeechRecognition\n","!pip install moviepy"]},{"cell_type":"markdown","metadata":{"id":"IEMww7OFS9Q2"},"source":["---\n","\n","\n","###***Util codes that need to be run before executing the codes in the main section***\n","\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"ND_AEie6TIP1"},"source":["#### Code to load images and preprocess them"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pue0ByWjTAyl"},"outputs":[],"source":["from PIL import Image\n","import requests\n","import torch\n","from torchvision import transforms\n","from torchvision.transforms.functional import InterpolationMode\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def loadImage(device, rawImage):\n","\n","    # imgUrl = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'\n","    # rawImage = Image.open(requests.get(imgUrl, stream=True).raw).convert('RGB')\n","    # imgUrl = '/content/drive/My Drive/My_Software_Projects/Input_Frames/time0_frame1.jpg'\n","    # rawImage = Image.open(imgUrl).convert('RGB')\n","    # rawImage = cv2.imread(imgUrl)\n","    rawImage = rawImage\n","    rawImage = cv2.cvtColor(rawImage, cv2.COLOR_BGR2RGB)\n","    rawImage = Image.fromarray(rawImage)\n","\n","\n","    w,h = rawImage.size\n","    display(rawImage.resize((w//5,h//5)))\n","    #cv2.imshow('image', raw_image)\n","\n","    #transform = transforms.Compose([\n","    #    transforms.Resize((imageSize,imageSize),interpolation=InterpolationMode.BICUBIC),\n","    #    transforms.ToTensor(),\n","    #    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n","    #    ])\n","    #image = transform(rawImage).unsqueeze(0).to(device)\n","\n","    return rawImage"]},{"cell_type":"markdown","metadata":{"id":"u3qv0a7qTzvW"},"source":["#### Make directories utli function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fxVq7BoRT1wM"},"outputs":[],"source":["import os\n","\n","def makeDirectory(path):\n","\n","    try:\n","\n","        os.makedirs(path, exist_ok=True)\n","        print(\"Directory '%s' created successfully\" % path)\n","\n","    except OSError as error:\n","\n","        print(\"Directory already exist\")\n","        pass"]},{"cell_type":"markdown","metadata":{"id":"KVSBA7dgT6gt"},"source":["#### Util function to write CSV File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ieN5Dq06T-KF"},"outputs":[],"source":["import csv\n","\n","def csvWriteRow(pathToCSVFile, rowData):\n","\n","  \"\"\"\n","  Util function to write a CSV File\n","\n","  pathToCSVFile: path to the CSV File\n","  rowData: Array data to be written\n","\n","  \"\"\"\n","\n","  # write the csv File\n","  with open(pathToCSVFile, 'a', newline='') as testWriteCSV:\n","\n","    csvWriter = csv.writer(testWriteCSV)\n","\n","    try:\n","\n","      csvWriter.writerow(rowData)\n","\n","    except UnicodeEncodeError as error:\n","\n","      pass"]},{"cell_type":"markdown","metadata":{"id":"_jBZ-IT7UEMN"},"source":["#### Function to get the audio to Text from a video file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CNDA33_uUIqN"},"outputs":[],"source":["import speech_recognition as sr\n","import moviepy.editor as mp\n","from moviepy.editor import VideoFileClip\n","from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n","\n","\n","def videoToText(pathToVideo):\n","  \"\"\"\n","  Util Function that converts video to text using the Google Speech to text API\n","\n","  :param pathToVideo : Path to the video file\n","  :return: List containing the audiototext per second\n","  \"\"\"\n","\n","  print (\"Converting speech to Text from Video..\")\n","  clip = VideoFileClip(pathToVideo)\n","\n","  numSecondsVideo = int(clip.duration)\n","  print(\"The video is {} seconds\".format(numSecondsVideo))\n","  lenList = list(range(0, numSecondsVideo + 1, 1))\n","\n","  resultDict = {}\n","  for i in range(len(lenList) - 1):\n","\n","    print(\"time = \" + str(i))\n","\n","    # clipping video into chunks of small sections to run through the speech APi\n","    ffmpeg_extract_subclip(pathToVideo, lenList[i] - 2 * (lenList[i] != 0), lenList[i + 1],\n","                           targetname=\"/content/drive/My Drive/My_Software_Projects/Intermediate_Folder/cut{}.mp4\".format(i + 1))\n","\n","    clip = mp.VideoFileClip(r\"/content/drive/My Drive/My_Software_Projects/Intermediate_Folder/cut{}.mp4\".format(i + 1))\n","\n","    # separating the audio from video\n","    clip.audio.write_audiofile(r\"/content/drive/My Drive/My_Software_Projects/Intermediate_Folder/converted{}.wav\".format(i + 1))\n","\n","    r = sr.Recognizer()\n","    audio = sr.AudioFile(\"/content/drive/My Drive/My_Software_Projects/Intermediate_Folder/converted{}.wav\".format(i + 1))\n","\n","    with audio as source:\n","      r.adjust_for_ambient_noise(source)\n","      audioFile = r.record(source)\n","\n","    try:\n","\n","      # feeding the sudio to the google speech to text API\n","      result = r.recognize_google(audioFile)\n","\n","    except sr.exceptions.UnknownValueError:\n","\n","      # store as no exception if the audio has not enough data to convert to text\n","      result = 'No transcript'\n","    resultDict['chunk{}'.format(i + 1)] = result\n","\n","  listText = [resultDict['chunk{}'.format(i + 1)] for i in range(len(resultDict))]\n","\n","  return listText"]},{"cell_type":"markdown","metadata":{"id":"Rnw2_KQYUioG"},"source":["---\n","\n","\n","### ***Main code Section to run inference of LENS on various modes and also dataset generation. (All the code cells above need to executed before the execution of the cells below)***\n","\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"Zgnf8YcWUtl1"},"source":["#### Code to run an inference on the Lens image Visual descriptions network from a video file (mode = all)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmCAC2j9UoT8"},"outputs":[],"source":["from PIL import Image\n","import requests\n","import torch\n","from lens import Lens, LensProcessor\n","import cv2\n","import time\n","import csv\n","\n","# question to be Asked (Text Prompt)\n","question = 'What is the facial expression of the person?'\n","\n","# csv object to be written to the csv File\n","# csvWrite = ['Timestamp', 'FrameID', 'FileName', 'CaptionedText', 'QAResult', 'AudioToText']\n","csvWrite = ['Timestamp', 'FrameID', 'FileName', 'Tags', 'Attributes', 'Caption', 'IntensiveCaptions', 'Prompts']\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","currentTime = time.strftime(\"%H_%M_%S\")\n","\n","#path to input Video File\n","pathToInputVideoFile = '/content/drive/My Drive/My_Software_Projects/Input_Video/InputVideo2.mp4'\n","\n","#path to the output Folder\n","pathToOutputFramesFolder = '/content/drive/My Drive/My_Software_Projects/Output/Output_Dataset_' + str(currentTime) + '/'\n","makeDirectory(pathToOutputFramesFolder)\n","\n","# path to the output text File\n","pathToOutputTxtFile = '/content/drive/My Drive/My_Software_Projects/Output_File/CaptionTextFile_LENS_' + str(currentTime) + '.txt'\n","pathToOutputCSVFile = '/content/drive/My Drive/My_Software_Projects/Output_File/CaptionCSVFile_LENS_' + str(currentTime) + '.csv'\n","\n","csvWriteRow(pathToCSVFile=pathToOutputCSVFile, rowData=csvWrite)\n","\n","\n","# Video to text conversion\n","# audioToTextList = videoToText(pathToVideo = pathToInputVideoFile)\n","# print (\"Conversion from speech to text successful.\")\n","\n","\n","print(\"Loading the Lens Model...\")\n","lens = Lens()\n","processor = LensProcessor()\n","print(\"Lens Model Successfully Loaded.\")\n","\n","print(\"Loading Input Video File.....\")\n","inputVideo = cv2.VideoCapture(pathToInputVideoFile)\n","print(\"Successfully Loaded input File\")\n","\n","\n","# Calculate the Frames per second (FPS)\n","print(\"Calculating Frames Per Second...\")\n","fps = round(inputVideo.get(cv2.CAP_PROP_FPS))\n","print('Fps = ' + str(fps))\n","\n","frameNumber = 0\n","timeStamp = 0\n","\n","print('Processing Frames...')\n","\n","\n","while True:\n","  # Processing Frames\n","  success, imageFrame = inputVideo.read()\n","\n","  if success:\n","    # increase the frame by 1\n","    frameNumber += 1\n","\n","    image = loadImage(device='cuda', rawImage=imageFrame)\n","\n","\n","    with torch.no_grad():\n","\n","      samples = processor([image],[question])\n","      output = lens(samples)\n","      prompts = output[\"prompts\"]\n","      tags = output[\"tags\"]\n","      attributes = output[\"attributes\"]\n","      caption = output[\"caption\"]\n","      intensiveCaptions = output[\"intensive_captions\"]\n","      #objects = output[\"objects\"]\n","      #print('answer: '+str(prompts[0]))\n","\n","\n","      #outputFrameFilePath = pathToOutputFramesFolder + str(answer[0]) + str(timeStamp) + str(frameNumber) + '.jpg'\n","      outputFrameFileName = 'time_' + str(timeStamp) + 'frame_' + str(frameNumber) + '.jpg'\n","      outputFrameFilePath = pathToOutputFramesFolder + outputFrameFileName\n","\n","      # write the frame\n","      cv2.imwrite(outputFrameFilePath, imageFrame)\n","\n","      # write the captions\n","      # writingText = \"TimeStamp = \" + str(timeStamp) +  \" Frame = \" + str(frameNumber) + \" Emotion : \" + str(answer[0])\n","      # utputTextFile.write(writingText)\n","\n","      # append a row to csvFile object\n","      # csvWrite = [str(timeStamp), str(frameNumber), outputFrameFileName, str(caption[0]), str(answer[0]), str(audioToTextList[timeStamp])]\n","      csvWrite = [str(timeStamp), str(frameNumber), outputFrameFileName, str(tags), str(attributes), str(caption), str(intensiveCaptions), str(prompts)]\n","      csvWriteRow(pathToCSVFile=pathToOutputCSVFile, rowData=csvWrite)\n","\n","      #with open(pathToOutputTxtFile, 'a') as testwritefile:\n","        #testwritefile.write(writingText + '\\n')\n","\n","      # outputTextFile.write('\\n')\n","\n","      print('Time = ' + str(timeStamp) + ' secs Frame = ' + str(frameNumber) + ' saved successfully')\n","\n","  else:\n","\n","    break\n","\n","  # every time you hit the last frame increase the time by 1 and reset the frames to 0\n","  if frameNumber == fps:\n","\n","    timeStamp += 1\n","    frameNumber = 0\n","\n","print(\"Frame successfully Processed.\")\n","inputVideo.release()\n","\n","# write the csv File\n","#with open(pathToOutputCSVFile, 'w', newline='') as testWriteCSV:\n","#  csvWriter = csv.writer(testWriteCSV)\n","#  csvWriter.writerows(csvWrite)"]},{"cell_type":"markdown","metadata":{"id":"w6wM8ippFmcn"},"source":["#### Code to run an inference on the Lens image Visual descriptions network and then feed into the Frozen LLM (mode = all)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ShfuNK-hFwOQ"},"outputs":[],"source":["from PIL import Image\n","import requests\n","import torch\n","from lens import Lens, LensProcessor\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","import cv2\n","import time\n","import csv\n","import re\n","\n","# question to be Asked (Text Prompt)\n","question = 'What is the emotion expressed?'\n","\n","# csv object to be written to the csv File\n","# csvWrite = ['Timestamp', 'FrameID', 'FileName', 'CaptionedText', 'QAResult', 'AudioToText']\n","csvWrite = ['Timestamp', 'FrameID', 'FileName', 'Tags', 'Attributes', 'Caption', 'IntensiveCaptions', 'Prompts', 'LLM Output']\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","currentTime = time.strftime(\"%H_%M_%S\")\n","\n","#path to input Video File\n","pathToInputVideoFile = '/content/drive/My Drive/My_Software_Projects/Input_Video/InputVideo2.mp4'\n","\n","#path to the output Folder\n","pathToOutputFramesFolder = '/content/drive/My Drive/My_Software_Projects/Output/Output_Dataset_' + str(currentTime) + '/'\n","makeDirectory(pathToOutputFramesFolder)\n","\n","# path to the output text File\n","pathToOutputTxtFile = '/content/drive/My Drive/My_Software_Projects/Output_File/CaptionTextFile_LENS_' + str(currentTime) + '.txt'\n","pathToOutputCSVFile = '/content/drive/My Drive/My_Software_Projects/Output_File/CaptionCSVFile_LENS_' + str(currentTime) + '.csv'\n","\n","csvWriteRow(pathToCSVFile=pathToOutputCSVFile, rowData=csvWrite)\n","\n","# regex to remove the <pad> and </s> from the output of the LLMs\n","remWord1 = re.compile('(\\s*)<pad>(\\s*)')\n","remWord2 = re.compile('(\\s*)</s>(\\s*)')\n","\n","\n","# Video to text conversion\n","# audioToTextList = videoToText(pathToVideo = pathToInputVideoFile)\n","# print (\"Conversion from speech to text successful.\")\n","\n","\n","print(\"Loading the Lens Model...\")\n","lens = Lens()\n","processor = LensProcessor()\n","print(\"Lens Model Successfully Loaded.\")\n","\n","print(\"Loading Input Video File.....\")\n","inputVideo = cv2.VideoCapture(pathToInputVideoFile)\n","print(\"Successfully Loaded input File\")\n","\n","\n","# Calculate the Frames per second (FPS)\n","print(\"Calculating Frames Per Second...\")\n","fps = round(inputVideo.get(cv2.CAP_PROP_FPS))\n","print('Fps = ' + str(fps))\n","\n","frameNumber = 0\n","timeStamp = 0\n","\n","print('Processing Frames...')\n","\n","\n","while True:\n","  # Processing Frames\n","  success, imageFrame = inputVideo.read()\n","\n","  if success:\n","    # increase the frame by 1\n","    frameNumber += 1\n","\n","    image = loadImage(device='cuda', rawImage=imageFrame)\n","\n","\n","    with torch.no_grad():\n","\n","      # infering the initial vision models like BLIP, CLIP\n","      samples = processor([image],[question])\n","      output = lens(samples)\n","\n","      # feeding the output of the vision models to a frozen LLM\n","      tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\",truncation_side = 'left',padding = True)\n","      LLMModel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n","      inputIds = tokenizer(samples[\"prompts\"], return_tensors=\"pt\").input_ids\n","      outputs = LLMModel.generate(inputIds)\n","\n","      test = str(tokenizer.decode(outputs[0]))\n","\n","      print ('Test = ' + test)\n","      LLMoutput = str(tokenizer.decode(outputs[0]))\n","      LLMoutput = remWord1.sub('', LLMoutput)\n","      LLMoutput = remWord2.sub('', LLMoutput)\n","\n","      print(LLMoutput)\n","\n","\n","      prompts = output[\"prompts\"]\n","      tags = output[\"tags\"]\n","      attributes = output[\"attributes\"]\n","      caption = output[\"caption\"]\n","      intensiveCaptions = output[\"intensive_captions\"]\n","\n","\n","\n","      #outputFrameFilePath = pathToOutputFramesFolder + str(answer[0]) + str(timeStamp) + str(frameNumber) + '.jpg'\n","      outputFrameFileName = 'time_' + str(timeStamp) + 'frame_' + str(frameNumber) + '.jpg'\n","      outputFrameFilePath = pathToOutputFramesFolder + outputFrameFileName\n","\n","      # write the frame\n","      cv2.imwrite(outputFrameFilePath, imageFrame)\n","\n","      # write the captions\n","      # writingText = \"TimeStamp = \" + str(timeStamp) +  \" Frame = \" + str(frameNumber) + \" Emotion : \" + str(answer[0])\n","      # utputTextFile.write(writingText)\n","\n","      # append a row to csvFile object\n","      # csvWrite = [str(timeStamp), str(frameNumber), outputFrameFileName, str(caption[0]), str(answer[0]), str(audioToTextList[timeStamp])]\n","      csvWrite = [str(timeStamp), str(frameNumber), outputFrameFileName, str(tags), str(attributes), str(caption), str(intensiveCaptions), str(prompts), str(LLMoutput)]\n","      csvWriteRow(pathToCSVFile=pathToOutputCSVFile, rowData=csvWrite)\n","\n","      print('Time = ' + str(timeStamp) + ' secs Frame = ' + str(frameNumber) + ' saved successfully')\n","\n","  else:\n","\n","    break\n","\n","  # every time you hit the last frame increase the time by 1 and reset the frames to 0\n","  if frameNumber == fps:\n","\n","    timeStamp += 1\n","    frameNumber = 0\n","\n","print(\"Frame successfully Processed.\")\n","inputVideo.release()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["Zgnf8YcWUtl1"],"authorship_tag":"ABX9TyPWtjKlXMEPqQcoBPMIw6yw"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}